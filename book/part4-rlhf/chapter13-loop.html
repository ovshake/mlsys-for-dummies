<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>rlhf_loop_pseudo.py - ML Systems for Dummies</title>


        <!-- Custom HTML head -->

        <meta name="description" content="From distributed primitives to production RLHF: A hands-on journey through ML infrastructure">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-9f1f5231.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-defe246b.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">ML Systems for Dummies</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="rlhf_loop_pseudopy"><a class="header" href="#rlhf_loop_pseudopy">rlhf_loop_pseudo.py</a></h1>
<blockquote>
<p>The complete RLHF training loop in pseudocode</p>
</blockquote>
<p>This script shows the exact computation flow of one RLHF training step, making it easy to understand what happens and when.</p>
<h2 id="what-it-does"><a class="header" href="#what-it-does">What It Does</a></h2>
<ol>
<li>Simulates all four models (Actor, Critic, Reward, Reference)</li>
<li>Walks through each step of the training loop</li>
<li>Shows tensor shapes and intermediate values</li>
<li>Demonstrates the complete PPO update</li>
</ol>
<h2 id="run-it"><a class="header" href="#run-it">Run It</a></h2>
<pre><code class="language-bash">python tutorial/part4-rlhf/chapter13-rlhf-flow/scripts/rlhf_loop_pseudo.py
</code></pre>
<h2 id="example-output"><a class="header" href="#example-output">Example Output</a></h2>
<pre><code>=== RLHF Training Loop Demo ===

Step 1: Sample prompts
  Batch size: 4
  Prompt shapes: (4, 64) tokens

Step 2: Generate responses (Actor)
  Actor forward pass...
  Generated tokens: (4, 128)
  Actor logits: (4, 128, 50257)
  Old log probs: (4, 128)

Step 3: Score responses (Reward Model)
  Reward model forward pass...
  Scores: [0.73, 0.45, 0.91, 0.62]

Step 4: Compute KL penalty (Reference)
  Reference forward pass...
  Reference log probs: (4, 128)
  KL divergence per token: (4, 128)
  Mean KL: 0.23

Step 5: Compute total rewards
  reward = reward_model_score - β * KL
  Total rewards: [0.50, 0.28, 0.75, 0.41]

Step 6: Compute advantages (Critic + GAE)
  Critic forward pass...
  Values: (4, 128)
  GAE advantages: (4, 128)

Step 7: PPO update
  Ratio = exp(new_log_prob - old_log_prob)
  Clipped ratio: clip(ratio, 0.8, 1.2)
  Actor loss: -0.042
  Critic loss: 0.156

  Update complete!
</code></pre>
<h2 id="the-core-loop"><a class="header" href="#the-core-loop">The Core Loop</a></h2>
<pre><code class="language-python">for batch in dataloader:
    # 1. Generate
    responses, old_logprobs = actor.generate(batch.prompts)

    # 2. Score
    rewards = reward_model(batch.prompts, responses)

    # 3. KL penalty
    ref_logprobs = reference(batch.prompts, responses)
    kl = old_logprobs - ref_logprobs
    rewards = rewards - beta * kl

    # 4. Advantages
    values = critic(batch.prompts, responses)
    advantages = gae(rewards, values)

    # 5. PPO update
    new_logprobs = actor(batch.prompts, responses)
    ratio = (new_logprobs - old_logprobs).exp()
    actor_loss = -torch.min(ratio * advantages,
                           ratio.clamp(0.8, 1.2) * advantages)
    critic_loss = (values - rewards) ** 2

    # 6. Backprop
    (actor_loss + critic_loss).backward()
    optimizer.step()
</code></pre>
<h2 id="source-code"><a class="header" href="#source-code">Source Code</a></h2>
<pre><code class="language-python">#!/usr/bin/env python3
"""
RLHF Training Loop Pseudocode

This script demonstrates the complete RLHF training loop with
detailed comments explaining each step.

This is PSEUDOCODE - not runnable without actual model implementations.
It's meant to illustrate the data flow and computations involved.

Usage:
    python rlhf_loop_pseudo.py
"""

from dataclasses import dataclass
from typing import List, Optional
import random
import math


@dataclass
class Prompt:
    """A training prompt."""
    text: str
    tokens: List[int]


@dataclass
class Response:
    """A generated response with metadata."""
    tokens: List[int]
    log_probs: List[float]  # From actor
    ref_log_probs: List[float]  # From reference
    values: List[float]  # From critic
    reward_score: float  # From reward model


@dataclass
class Experience:
    """One token of experience for PPO."""
    token: int
    log_prob: float
    ref_log_prob: float
    value: float
    reward: float
    advantage: float


def rlhf_training_step(
    prompts: List[Prompt],
    actor,  # The policy model being trained
    critic,  # The value function
    reward_model,  # Frozen reward model
    reference,  # Frozen reference policy
    kl_coef: float = 0.02,
    gamma: float = 1.0,
    lam: float = 0.95,
    clip_epsilon: float = 0.2,
) -&gt; dict:
    """
    One step of RLHF training.

    This function shows the complete data flow through all four models.
    """
    print("=" * 70)
    print(" RLHF TRAINING STEP")
    print("=" * 70)

    # =========================================================================
    # STEP 1: Generate Responses (Actor)
    # =========================================================================
    print("\n[Step 1] GENERATE RESPONSES")
    print("-" * 50)

    responses = []
    for prompt in prompts:
        # Generate response from actor
        # In reality: autoregressive generation with temperature sampling
        response_tokens = generate_response(actor, prompt)

        # Get log probabilities from actor
        actor_log_probs = get_log_probs(actor, prompt.tokens, response_tokens)

        responses.append(Response(
            tokens=response_tokens,
            log_probs=actor_log_probs,
            ref_log_probs=[],  # Filled in step 3
            values=[],  # Filled in step 4
            reward_score=0,  # Filled in step 2
        ))
        print(f"  Generated {len(response_tokens)} tokens for prompt")

    # =========================================================================
    # STEP 2: Score Responses (Reward Model)
    # =========================================================================
    print("\n[Step 2] SCORE RESPONSES (Reward Model)")
    print("-" * 50)

    for i, (prompt, response) in enumerate(zip(prompts, responses)):
        # Get reward score for complete response
        # In reality: forward pass through reward model
        full_sequence = prompt.tokens + response.tokens
        response.reward_score = score_response(reward_model, full_sequence)
        print(f"  Response {i}: reward = {response.reward_score:.3f}")

    # =========================================================================
    # STEP 3: Compute KL Penalty (Reference Model)
    # =========================================================================
    print("\n[Step 3] COMPUTE KL PENALTY (Reference)")
    print("-" * 50)

    total_kl = 0
    for prompt, response in zip(prompts, responses):
        # Get reference log probabilities
        response.ref_log_probs = get_log_probs(
            reference, prompt.tokens, response.tokens
        )

        # Compute per-token KL divergence
        kl_per_token = []
        for actor_lp, ref_lp in zip(response.log_probs, response.ref_log_probs):
            # KL = exp(actor_lp) * (actor_lp - ref_lp)
            # Simplified: just the log ratio for penalty
            kl = actor_lp - ref_lp
            kl_per_token.append(kl)

        avg_kl = sum(kl_per_token) / len(kl_per_token)
        total_kl += avg_kl

    avg_kl = total_kl / len(responses)
    print(f"  Average KL divergence: {avg_kl:.4f}")

    # =========================================================================
    # STEP 4: Compute Values (Critic)
    # =========================================================================
    print("\n[Step 4] COMPUTE VALUES (Critic)")
    print("-" * 50)

    for prompt, response in zip(prompts, responses):
        # Get value estimates for each token position
        # In reality: forward pass through critic
        response.values = get_values(critic, prompt.tokens, response.tokens)
        print(f"  Values computed: mean={sum(response.values)/len(response.values):.3f}")

    # =========================================================================
    # STEP 5: Compute Rewards with KL Penalty
    # =========================================================================
    print("\n[Step 5] COMPUTE REWARDS WITH KL PENALTY")
    print("-" * 50)

    all_experiences = []

    for prompt, response in zip(prompts, responses):
        experiences = []

        for t in range(len(response.tokens)):
            # Per-token KL penalty
            kl_penalty = kl_coef * (response.log_probs[t] - response.ref_log_probs[t])

            # Reward: only at last token, minus KL at every token
            if t == len(response.tokens) - 1:
                token_reward = response.reward_score - kl_penalty
            else:
                token_reward = -kl_penalty  # Just KL penalty for non-final tokens

            experiences.append(Experience(
                token=response.tokens[t],
                log_prob=response.log_probs[t],
                ref_log_prob=response.ref_log_probs[t],
                value=response.values[t],
                reward=token_reward,
                advantage=0,  # Computed in step 6
            ))

        all_experiences.append(experiences)
        final_reward = experiences[-1].reward
        print(f"  Final token reward: {final_reward:.3f} "
              f"(score={response.reward_score:.3f}, kl_penalty included)")

    # =========================================================================
    # STEP 6: Compute GAE Advantages
    # =========================================================================
    print("\n[Step 6] COMPUTE GAE ADVANTAGES")
    print("-" * 50)

    for experiences in all_experiences:
        # GAE computation (backwards)
        gae = 0
        for t in reversed(range(len(experiences))):
            exp = experiences[t]

            if t == len(experiences) - 1:
                next_value = 0  # Terminal state
            else:
                next_value = experiences[t + 1].value

            # TD error
            delta = exp.reward + gamma * next_value - exp.value

            # GAE
            gae = delta + gamma * lam * gae
            exp.advantage = gae

        # Normalize advantages
        advantages = [e.advantage for e in experiences]
        mean_adv = sum(advantages) / len(advantages)
        std_adv = (sum((a - mean_adv) ** 2 for a in advantages) / len(advantages)) ** 0.5
        for exp in experiences:
            exp.advantage = (exp.advantage - mean_adv) / (std_adv + 1e-8)

        print(f"  Advantages computed and normalized")

    # =========================================================================
    # STEP 7: PPO Update
    # =========================================================================
    print("\n[Step 7] PPO UPDATE")
    print("-" * 50)

    # Flatten all experiences
    flat_experiences = [exp for exps in all_experiences for exp in exps]

    # Compute PPO losses
    policy_losses = []
    value_losses = []
    clip_fractions = []

    for exp in flat_experiences:
        # New log probability (after potential update)
        # In reality: forward pass through updated actor
        new_log_prob = exp.log_prob  # Placeholder

        # Probability ratio
        ratio = math.exp(new_log_prob - exp.log_prob)

        # Clipped objective
        unclipped = ratio * exp.advantage
        clipped = max(min(ratio, 1 + clip_epsilon), 1 - clip_epsilon) * exp.advantage

        policy_loss = -min(unclipped, clipped)
        policy_losses.append(policy_loss)

        # Value loss
        # In reality: new value prediction
        new_value = exp.value  # Placeholder
        value_loss = (new_value - (exp.reward + gamma * 0)) ** 2  # Simplified
        value_losses.append(value_loss)

        # Track clipping
        if abs(ratio - 1) &gt; clip_epsilon:
            clip_fractions.append(1)
        else:
            clip_fractions.append(0)

    avg_policy_loss = sum(policy_losses) / len(policy_losses)
    avg_value_loss = sum(value_losses) / len(value_losses)
    avg_clip_frac = sum(clip_fractions) / len(clip_fractions)

    print(f"  Policy loss: {avg_policy_loss:.4f}")
    print(f"  Value loss: {avg_value_loss:.4f}")
    print(f"  Clip fraction: {avg_clip_frac:.2%}")

    # =========================================================================
    # Summary
    # =========================================================================
    print("\n" + "=" * 70)
    print(" STEP SUMMARY")
    print("=" * 70)
    print(f"""
Models used:
  - Actor: Generated {sum(len(r.tokens) for r in responses)} total tokens
  - Reward: Scored {len(responses)} responses
  - Reference: Computed KL for {len(responses)} responses
  - Critic: Estimated values for {sum(len(r.tokens) for r in responses)} tokens

Losses:
  - Policy loss: {avg_policy_loss:.4f}
  - Value loss: {avg_value_loss:.4f}

KL penalty:
  - Average KL: {avg_kl:.4f}
  - KL coefficient: {kl_coef}
  - Total KL penalty: {avg_kl * kl_coef:.4f}
""")

    return {
        'policy_loss': avg_policy_loss,
        'value_loss': avg_value_loss,
        'kl': avg_kl,
        'clip_fraction': avg_clip_frac,
    }


# =============================================================================
# Placeholder functions (would be real model calls in practice)
# =============================================================================

def generate_response(actor, prompt: Prompt) -&gt; List[int]:
    """Generate response tokens from actor."""
    # Simulated: random tokens
    length = random.randint(10, 30)
    return [random.randint(0, 999) for _ in range(length)]


def get_log_probs(model, prompt_tokens: List[int],
                   response_tokens: List[int]) -&gt; List[float]:
    """Get log probabilities from model."""
    # Simulated: random log probs
    return [random.uniform(-3, -0.1) for _ in response_tokens]


def score_response(reward_model, tokens: List[int]) -&gt; float:
    """Get reward score from reward model."""
    # Simulated: random score
    return random.uniform(-1, 1)


def get_values(critic, prompt_tokens: List[int],
                response_tokens: List[int]) -&gt; List[float]:
    """Get value estimates from critic."""
    # Simulated: decreasing values
    n = len(response_tokens)
    return [0.5 * (n - i) / n for i in range(n)]


def main():
    print("╔" + "═" * 68 + "╗")
    print("║" + " RLHF TRAINING LOOP DEMONSTRATION".center(68) + "║")
    print("╚" + "═" * 68 + "╝")

    # Create sample prompts
    prompts = [
        Prompt("What is the capital of France?", [1, 2, 3, 4, 5]),
        Prompt("Explain quantum computing.", [6, 7, 8, 9]),
        Prompt("Write a haiku about programming.", [10, 11, 12, 13, 14]),
        Prompt("What is machine learning?", [15, 16, 17]),
    ]

    print(f"\nBatch size: {len(prompts)} prompts")

    # Run one training step
    stats = rlhf_training_step(
        prompts=prompts,
        actor=None,  # Placeholder
        critic=None,
        reward_model=None,
        reference=None,
        kl_coef=0.02,
    )

    # Explain the process
    print("\n" + "=" * 70)
    print(" WHAT JUST HAPPENED")
    print("=" * 70)
    print("""
This simulated one complete RLHF training step:

1. GENERATION: Actor generated responses for each prompt
2. SCORING: Reward model evaluated response quality
3. KL COMPUTATION: Reference model computed divergence penalty
4. VALUE ESTIMATION: Critic predicted expected rewards
5. ADVANTAGE COMPUTATION: GAE combined rewards and values
6. PPO UPDATE: Actor and critic weights updated

In production, this happens with:
- Real neural network forward/backward passes
- GPU tensor operations
- Distributed training across multiple devices
- Gradient accumulation and synchronization

The key insight: RLHF is just PPO with:
- Reward from a learned reward model
- KL penalty to stay close to reference
- Four models instead of just actor-critic
""")


if __name__ == "__main__":
    main()
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../part4-rlhf/chapter13.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../part4-rlhf/chapter13-reward.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../part4-rlhf/chapter13.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../part4-rlhf/chapter13-reward.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
